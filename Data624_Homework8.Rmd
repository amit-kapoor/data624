---
title: "Data624 - Homework8"
author: "Amit Kapoor"
date: "4/19/2021"
output:
  pdf_document:
    latex_engine: xelatex
    toc: yes
  html_document:
    highlight: pygments
    number_sections: no
    theme: flatly
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center", fig.height = 5, fig.width = 8, warning = FALSE)
```



```{r,warning=FALSE,message=FALSE}

library(AppliedPredictiveModeling)
library(tidyverse)
library(caret)
library(mlbench)
library(naniar)
```



# Exercise 7.2
Friedman (1991) introduced several benchmark data sets create by simulation. One of these simulations used the following nonlinear equation to create data:

\[y = 10 sin(\pi x_1 x_2) + 20(x_3 − 0.5)^2 + 10x_4 + 5x_5 + N(0, \sigma^2)\]

where the x values are random variables uniformly distributed between [0, 1] (there are also 5 other non-informative variables also created in the simulation). The package `mlbench` contains a function called `mlbench.friedman1` that simulates these data: 


```{r}
set.seed(200)
trainingData <- mlbench.friedman1(200, sd=1)

## We convert the 'x' data from a matrix to a data frame
## One reason is that this will give the columns names.
trainingData$x <- data.frame(trainingData$x)

# featurePlot
featurePlot(trainingData$x, trainingData$y)
```



```{r}
glimpse(trainingData$x)
```



```{r}
## This creates a list with a vector 'y' and a matrix
## of predictors 'x'. Also simulate a large test set to
## estimate the true error rate with good precision:
testData <- mlbench.friedman1(5000, sd=1)
testData$x <- data.frame(testData$x)

glimpse(testData)
```



## Models
Tune several models on these data.


### K-Nearest Neighbors

The KNN algorithm assumes that similar things exist in close proximity. In other words, kNN approach simply predicts a new sample using the K-closest samples from the training set. Here we will use training using knn method on training data and find the besttune k value.


```{r}
set.seed(317)
knnfit <- train(trainingData$x,
                trainingData$y,
                method = "knn",
                preProcess = c("center","scale"),
                tuneLength = 20,
                trControl = trainControl(method = "cv"))

knnfit
```


```{r}
# final parameters
knnfit$bestTune
```

```{r}
# plot RMSE
plot(knnfit)
```


```{r}
# plot variable importance
plot(varImp(knnfit))
```


```{r}
data.frame(Rsquared=knnfit[["results"]][["Rsquared"]][as.numeric(rownames(knnfit$bestTune))],
           RMSE=knnfit[["results"]][["RMSE"]][as.numeric(rownames(knnfit$bestTune))])
```

It is evident here that the best value of K is 9 which resulted Rsquared as 0.69 and RMSE as 3.01. Also the top 5 top predictors are X4, X1, X2, X5 and X3.



### Support Vector Machines

The objective of the support vector machine algo is to find a hyperplane in an N-dimensional space (N being the number of features) that classifies the data points. Here we will use training using svmRadial method .


```{r}
set.seed(317)
svmfit <- train(trainingData$x,
                trainingData$y,
                method = "svmRadial",
                preProcess = c("center","scale"),
                tuneLength = 20,
                trControl = trainControl(method = "cv"))

svmfit
```



```{r}
svmfit$finalModel
```


```{r}
# plot RMSE
plot(svmfit)
```


```{r}
# plot variable importance
plot(varImp(svmfit))
```



```{r}
data.frame(Rsquared=svmfit[["results"]][["Rsquared"]][as.numeric(rownames(svmfit$bestTune))],
           RMSE=svmfit[["results"]][["RMSE"]][as.numeric(rownames(svmfit$bestTune))])
```

So we can see here that best SVM model produced Rsquared as 0.87 and RMSE as 1.86. Tuning parameter 'sigma' was held constant at a value of 0.063 RMSE was used to select the optimal model using the smallest value. Also the top 5 top predictors are X4, X1, X2, X5 and X3.



### Multivariate Adaptive Regression Splines

MARS creates a piecewise linear model which provides an intuitive stepping block into non-linearity after grasping the concept of multiple linear regression. MARS provided a convenient approach to capture the nonlinear relationships in the data by assessing cutpoints (knots) similar to step functions. The procedure assesses each data point for each predictor as a knot and creates a linear regression model with the candidate features.


```{r}
set.seed(317)
marsGrid <- expand.grid(.degree=1:2, .nprune=2:38)
marsfit <- train(trainingData$x,
                trainingData$y,
                method = "earth",
                preProcess = c("center","scale"),
                tuneGrid = marsGrid,
                trControl = trainControl(method = "cv"))

marsfit
```



```{r}
# final parameters
marsfit$bestTune
```


```{r}
# plot RMSE
plot(marsfit)
```

```{r}
# plot variable importance
plot(varImp(marsfit))
```

```{r}
data.frame(Rsquared=marsfit[["results"]][["Rsquared"]][as.numeric(rownames(marsfit$bestTune))],
           RMSE=marsfit[["results"]][["RMSE"]][as.numeric(rownames(marsfit$bestTune))])
```

RMSE was used to select the optimal model using the smallest value. The final values used for the model were nprune = 14 and degree = 2 that resulted Rsquared as 0.94 and RMSE as 1.20. So far we can see MARS model has a best fit on training data comparing with KNN and SVM. Also the top predictors are X1, X4, X2 and X5.


### Neural Networks

Neural Networks are nonlinear regression techniques inspired by theories about how the brain works. The outcome is modeled by an intermediary set of unobserved variables (hidden variables). These hidden units are linear combinations of the original predictors.


```{r}
set.seed(317)
nnetGrid <- expand.grid(.decay=c(0,0.01,.1),
                        .size=c(1:10),
                        .bag=FALSE)

nnetfit <- train(trainingData$x,
                 trainingData$y,
                 method = "avNNet", 
                 tuneGrid = nnetGrid, 
                 preProcess = c("center","scale"), 
                 linout = TRUE,
                 trace = FALSE,
                 MaxNWts =10 * (ncol(trainingData$x)+1) +10+1,
                 maxit=500)

nnetfit
```


```{r}
# final parameters
nnetfit$bestTune
```


```{r}
# plot RMSE
plot(nnetfit)
```


```{r}
# plot variable importance
plot(varImp(nnetfit))
```


```{r}
data.frame(Rsquared=nnetfit[["results"]][["Rsquared"]][as.numeric(rownames(nnetfit$bestTune))],
           RMSE=nnetfit[["results"]][["RMSE"]][as.numeric(rownames(nnetfit$bestTune))])
```

RMSE was used to select the optimal model using the smallest value. The final values used for the model were size = 4, decay = 0.1 and bag = FALSE that resulted the Rsquared 0.75 and RMSE as 2.50. The top predictors come up are X4, X1, X2, X5 and X3.


## Performance
Which models appear to give the best performance? Does MARS select the informative predictors (those named X1–X5)?



```{r}
set.seed(317)
knn.pred <- predict(knnfit, newdata = testData$x)
svm.pred <- predict(svmfit, newdata = testData$x)
mars.pred <- predict(marsfit, newdata = testData$x)
nnet.pred <- predict(nnetfit, newdata = testData$x)


data.frame(rbind(KNN=postResample(pred=knn.pred,obs = testData$y),
                 SVM=postResample(pred=svm.pred,obs = testData$y),
                 MARS=postResample(pred=mars.pred,obs = testData$y),
                 NNET=postResample(pred=nnet.pred,obs = testData$y)))

```



From the results, it is evident that the best model is MARS with $R^2$ = 0.93 and min RMSE = 1.28 on test data. The MARS does select the informative predictors X1-X5. 



# Exercise 7.5
Exercise 6.3 describes data for a chemical manufacturing process. Use the same data imputation, data splitting, and pre-processing steps as before and train several nonlinear regression models


```{r}
data(ChemicalManufacturingProcess)
```


```{r}
glimpse(ChemicalManufacturingProcess)
```



The matrix `processPredictors` contains the 57 predictors (12 describing the input biological material and 45 describing the process predictors) for the 176 manufacturing runs. yield contains the percent yield for each run.


We will first see all the variables having any of the missing values. We have used below complete.cases() function to find the the missing values.


```{r}
# columns having missing values
colnames(ChemicalManufacturingProcess)[!complete.cases(t(ChemicalManufacturingProcess))]
```


So there are 28 columns having missing values. Here is the plot for missing values of all the predictors.


```{r fig1, fig.height=10, fig.width=10}
gg_miss_var(ChemicalManufacturingProcess[,-c(1)]) + labs(y = "Sorted by Missing values")
```

We will next use preProcess() method to impute the missing values using knnImpute (K nearest neighbor).

```{r}
pre.proc <- preProcess(ChemicalManufacturingProcess[,c(-1)], method = "knnImpute")
chem_df <- predict(pre.proc, ChemicalManufacturingProcess[,c(-1)])
```


```{r fig2}
# columns having missing values
colnames(chem_df)[!complete.cases(t(chem_df))]
```



We will first filter out the predictors that have low frequencies using the `nearZeroVar` function from the caret package. After applying this function we see 1 column is removed and 56  predictors are left for modeling.

```{r}
chem.remove.pred <- nearZeroVar(chem_df)
chem_df <- chem_df[,-chem.remove.pred]
length(chem.remove.pred) %>% paste('columns are removed. ', dim(chem_df)[2], ' predictors are left for modeling.') %>% print()
```


We will now look into pairwise correlation above 0.90 and remove the predictors having correlation with cutoff 0.90.


```{r}
chem.corr.90 <- findCorrelation(cor(chem_df), cutoff=0.90)
chem_df <- chem_df[,-chem.corr.90]
length(chem.corr.90) %>% paste('columns having correlation 0.90 or more are removed. ', dim(chem_df)[2], ' predictors are left for modeling.') %>% print()
```


Next step is to split the data in training and testing set. We reserve 70% for training and 30% for testing. After split we will fit elastic net model.


```{r}
set.seed(786)

pre.proc <- preProcess(chem_df, method = c("center", "scale"))
chem_df <- predict(pre.proc, chem_df)

# partition
chem.part <- createDataPartition(ChemicalManufacturingProcess$Yield, p=0.80, list = FALSE)

# predictor
X.train <- chem_df[chem.part,]
X.test <- chem_df[-chem.part,]

# response 
y.train <- ChemicalManufacturingProcess$Yield[chem.part]
y.test <- ChemicalManufacturingProcess$Yield[-chem.part]


```



## (a)
Which nonlinear regression model gives the optimal resampling and test set performance




### K-Nearest Neighbors


```{r}
set.seed(317)
knnmodel <- train(X.train,
                y.train,
                method = "knn",
                preProcess = c("center","scale"),
                tuneLength = 10,
                trControl = trainControl(method = "cv"))

knnmodel
```


```{r}
# final parameters
knnmodel$bestTune
```

```{r}
# plot RMSE
plot(knnmodel)
```


```{r}
# plot variable importance
plot(varImp(knnmodel), top = 20)
```


```{r}
data.frame(Rsquared=knnmodel[["results"]][["Rsquared"]][as.numeric(rownames(knnmodel$bestTune))],
           RMSE=knnmodel[["results"]][["RMSE"]][as.numeric(rownames(knnmodel$bestTune))])
```


The best tune parameter for the KNN model that resulted in the smallest root mean squared error is 5 which has RMSE = 1.27, and $R^2$= 0.52. Also we can see quite a few top informative predictors from this model.

### Support Vector Machines


```{r}
set.seed(317)
svmmodel <- train(X.train,
                y.train,
                method = "svmRadial",
                preProcess = c("center","scale"),
                tuneLength = 10,
                trControl = trainControl(method = "cv"))

svmmodel
```



```{r}
svmmodel$finalModel
```


```{r}
# plot RMSE
plot(svmmodel)
```


```{r}
# plot variable importance
plot(varImp(svmmodel), top = 20)
```



```{r}
data.frame(Rsquared=svmmodel[["results"]][["Rsquared"]][as.numeric(rownames(svmmodel$bestTune))],
           RMSE=svmmodel[["results"]][["RMSE"]][as.numeric(rownames(svmmodel$bestTune))])
```



So we can see here that best SVM model produced Rsquared as 0.68 and RMSE as 1.11. Tuning parameter 'sigma' was held constant at a value of 0.016 RMSE was used to select the optimal model using the smallest value. The final values used for the model were sigma = 0.01657003 and C = 16.


### Multivariate Adaptive Regression Splines


```{r, warning=FALSE}
set.seed(317)
marsGrid2 <- expand.grid(.degree=1:2, .nprune=2:38)
marsmodel <- train(X.train,
                y.train,
                method = "earth",
                #preProcess = c("center","scale"),
                tuneGrid = marsGrid2,
                trControl = trainControl(method = "cv"))

marsmodel
```



```{r}
# final parameters
marsmodel$bestTune
```


```{r}
plot(marsmodel)
```



```{r}
# plot variable importance
plot(varImp(marsmodel), top=20)
```

```{r}
data.frame(Rsquared=marsmodel[["results"]][["Rsquared"]][as.numeric(rownames(marsmodel$bestTune))],
           RMSE=marsmodel[["results"]][["RMSE"]][as.numeric(rownames(marsmodel$bestTune))])
```



RMSE was used to select the optimal model using the smallest value. The final values used for the model were nprune = 11 and degree = 1 that resulted Rsquared as 0.59 and RMSE as 1.29. So far we can see SVM model has a best fit on training data comparing with KNN and MARS. Also we see 4 top predictors in this model.



### Neural Networks


```{r, warning=FALSE}
set.seed(317)
nnetGrid2 <- expand.grid(.decay=c(0,0.01,.1),
                        .size=c(1:5),
                        .bag=FALSE)

nnetmodel <- train(X.train,
                 y.train,
                 method = "avNNet", 
                 tuneGrid = nnetGrid2, 
                 #preProcess = c("center","scale"), 
                 trControl = trainControl(method = "cv"),
                 linout = TRUE,
                 trace = FALSE,
                 MaxNWts =5 * (ncol(X.train)+1) +5+1,
                 maxit=500)

nnetmodel
```


```{r}
# final parameters
nnetmodel$bestTune
```



```{r}
# plot RMSE
plot(nnetmodel)
```



```{r}
# plot variable importance
plot(varImp(nnetmodel), top=20)
```


```{r}
data.frame(Rsquared=nnetmodel[["results"]][["Rsquared"]][as.numeric(rownames(nnetmodel$bestTune))],
           RMSE=nnetmodel[["results"]][["RMSE"]][as.numeric(rownames(nnetmodel$bestTune))])
```


RMSE was used to select the optimal model using the smallest value. Tuning parameter 'bag' was held constant at a value of FALSE. The final values used for the model were size = 2, decay = 0.01 and bag = FALSE that resulted the Rsquared 0.36 and RMSE as 1.89.


### Optimal resampling

Now we will use resampling method to get the performance metrics and analyze the results to select the best fit model here. So far SVM model produced the best results.



```{r}
set.seed(317)
summary(resamples(list(KNN=knnmodel, SVM=svmmodel, MARS=marsmodel, NNET=nnetmodel)))
```


### Test set performance

```{r}
set.seed(317)
knnpred <- predict(knnmodel, newdata = X.test)
svmpred <- predict(svmmodel, newdata = X.test)
marspred <- predict(marsmodel, newdata = X.test)
nnetpred <- predict(nnetmodel, newdata = X.test)


data.frame(rbind(KNN=postResample(pred=knnpred,obs = y.test),
                 SVM=postResample(pred=svmpred,obs = y.test),
                 MARS=postResample(pred=marspred,obs = y.test),
                 NNET=postResample(pred=nnetpred,obs = y.test)))

```


From the results, we can conclude that the SVM model predicted the test response with best accuracy $R^2$=0.62, RMSE=1.02 and MAE=0.86


## (b)
Which predictors are most important in the optimal nonlinear regression model? Do either the biological or process variables dominate the list? How do the top ten important predictors compare to the top ten predictors from the optimal linear model?


Here is the list of top 10 most important predictors from SVM model. The `caret:varImp` calculates the variable importance for regression that shows the relationship between each predictor and the output from linear model fit. We can see below the most important contribution variable is `ManufacturingProcess32` and hence ManufacturingProcess dominate the list.



```{r}
# plot variable importance
varImp(svmmodel, top=10)
```


It was stated earlier that elasticnwt model that best fitted the data among linear models. We can see here too that ManufacturingProcess variables dominates the list but ranks seem different between linear and non linear models.


```{r}
# tune elastic net model 
chem.enet.fit <- train(x=X.train, 
                       y=y.train,
                       method="glmnet",
                       metric="Rsquared",
                       trControl=trainControl(method = "cv",number=10),
                       tuneLength = 5
                 )


varImp(chem.enet.fit)
```






## (c)
Explore the relationships between the top predictors and the response for the predictors that are unique to the optimal nonlinear regression model. Do these plots reveal intuition about the biological or process predictors and their relationship with yield?


We will now get the top 10 predictors, arrange it in order and then draw the `featureplot` to explore the visualization.


```{r}
# predictors importance
vimp <- varImp(svmmodel)$importance
# top 10 predictors
top10.vars <- head(rownames(vimp)[order(-vimp$Overall)], 10)
as.data.frame(top10.vars)
```

```{r fig.width=12}
X <- ChemicalManufacturingProcess[,top10.vars]
Y <- ChemicalManufacturingProcess$Yield

featurePlot(X,Y)
```


From the plots above, it is apparent that for SVM model (optimal model) the top predictors have mostly linear relationship with the response `Yield`. Increasing the features like `ManufacturingProcess32` or `BiologicalMaterial06` increases the response while increasing features like ManufacturingProcess13 cause decrease in response variable.













