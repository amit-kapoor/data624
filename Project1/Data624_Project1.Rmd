---
title: "Data624 - Project1"
author: "Amit Kapoor"
date: "3/28/2021"
output:
  pdf_document:
    toc: yes
  html_document:
    highlight: pygments
    number_sections: no
    theme: flatly
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Overview

This project includes 3 time series dataset and requires to select best forecasting model for all 3 datasets.

* Part A - ATM Forecast 
* Part B - Forecasting Power
* Part C - Waterflow Pipe


```{r lib, include=FALSE, message=FALSE}
# Libraries
library(readxl)
library(ggplot2)
library(tidyverse)
library(psych)
library(forecast)
library(gridExtra)
library(fpp2)
library(urca)
library(kableExtra)
library(lubridate)
library(openxlsx)
```



# Part A - ATM Forecast 

The dataset contains cash withdrawals from 4 different ATM machines from May 2009 to Apr 2010. The variable ‘Cash’ is provided in hundreds of dollars and data is in a single file. Before starting our analysis we will first download the excel from github and then read it through read_excel.


## Exploratory Analysis

```{r warning=FALSE}
temp.file <- tempfile(fileext = ".xlsx")
download.file(url="https://github.com/amit-kapoor/data624/blob/main/Project1/ATM624Data.xlsx?raw=true", 
              destfile = temp.file, 
              mode = "wb", 
              quiet = TRUE)
atm.data <- read_excel(temp.file, skip=0, col_types = c("date","text","numeric"))

glimpse(atm.data)
```


```{r}
# rows missing values
atm.data[!complete.cases(atm.data),]
```


```{r}
ggplot(atm.data[complete.cases(atm.data),] , aes(x=DATE, y=Cash, col=ATM )) + 
  geom_line(show.legend = FALSE) + 
  facet_wrap(~ATM, ncol=1, scales = "free")
```


```{r}
ggplot(atm.data[complete.cases(atm.data),] , aes(x=Cash )) + 
  geom_histogram(bins=20) + 
  facet_grid(cols=vars(ATM), scales = "free")
```


```{r}
# consider complete cases
atm.comp <- atm.data[complete.cases(atm.data),]
# pivot wider with cols from 4 ATMs and their values as Cash
atm.comp <- atm.comp %>% pivot_wider(names_from = ATM, values_from = Cash)
head(atm.comp)
```

```{r}
# summary
atm.comp %>% select(-DATE) %>% summary()
```



Per above exploratory analysis, all ATMs show different patterns. We would perform forecasting for each ATM separately.

* ATM1 and ATM2 shows similar pattern (approx.) throughout the time. ATM1 and ATM2 have 3 and 2 missing entries respectively.
* ATM3 appears to become online in last 3 days only and rest of days appears inactive. So tha data available for this ATM is very limited.
* ATM4 requires replacement for outlier and we can assume that one day spike of cash withdrawal is unique. It has an outlier showing withdrawl amount 10920.


## Data Cleaning

For this part we will first apply ts() function to get required time series. Next step is to apply tsclean function that will handle missing data along with outliers. To estimate missing values and outlier replacements, this function uses linear interpolation on the (possibly seasonally adjusted) series. Once we get the clean data we will use pivot_longer to get the dataframe in its original form.

```{r}
atm.ts <- ts(atm.comp %>% select(-DATE))
head(atm.ts)
```



```{r}
# apply tsclean
atm.ts.cln <- sapply(X=atm.ts, tsclean)
atm.ts.cln %>% summary()
```

If we compare this summary with previous one of original data, ATM1 and ATM2 has nomore NAs and ATM4 outlier value (10919.762) is handled and now the max value is 1712.075.

```{r}
# convert into data frame, pivot longer , arrange by ATM and bind with dates
atm.new <- as.data.frame(atm.ts.cln) %>% 
  pivot_longer(everything(), names_to = "ATM", values_to = "Cash") %>% 
  arrange(ATM)

atm.new <- cbind(DATE = seq(as.Date("2009-05-1"), as.Date("2010-04-30"), length.out=365), 
                 atm.new)

head(atm.new)
```




```{r}
ggplot(atm.new , aes(x=DATE, y=Cash, col=ATM )) + 
  geom_line(show.legend = FALSE) + 
  facet_wrap(~ATM, ncol=1, scales = "free")
```

Though above plot doesn't show much differences for ATM1,2,3 but tsclean handled the ATM4 data very well after replacing the outlier.


## Time Series


Function to plot forecast for various models.

```{r func-fcst}
# function to plot forecast(s)
atm.forecast <- function(timeseries) {
  # lambda value
  lambda <- BoxCox.lambda(timeseries)
  # models for forecast
  hw.model <- timeseries %>% hw(h=31, seasonal = "additive", lambda = lambda, damped = TRUE)
  ets.model <- timeseries %>% ets(lambda = lambda)
  arima.model <- timeseries %>% auto.arima(lambda = lambda)
  # forecast
  atm.hw.fcst <- forecast(hw.model, h=31)
  atm.ets.fcst <- forecast(ets.model, h=31)
  atm.arima.fcst <- forecast(arima.model, h=31)
  # plot forecasts
  p1 <- autoplot(timeseries) + 
    autolayer(atm.hw.fcst, PI=FALSE, series="Holt-Winters") + 
    autolayer(atm.ets.fcst, PI=FALSE, series="ETS") + 
    autolayer(atm.arima.fcst, PI=FALSE, series="ARIMA") + 
    theme(legend.position = "top") + 
    ylab("Cash Withdrawl") 
  # zoom in plot
  p2 <- p1 + 
    labs(title = "Zoom in ") + 
    xlim(c(51,56))
  
  grid.arrange(p1,p2,ncol=1)

}
```


Function to calculate RMSEs for various models.


```{r rmse}
model_accuracy <- function(timeseries, atm_num) {
  # lambda value
  lambda <- BoxCox.lambda(timeseries)
  
  # split the data to train and test
  train <- window(timeseries, end=c(40, 3))
  test <- window(timeseries, start=c(40, 4))
  
  # models for forecast
  hw.model <- train %>% hw(h=length(train), seasonal = "additive", lambda = lambda, damped = TRUE)
  ets.model <- train %>% ets(model='ANA', lambda = lambda)
  
  # Arima model
  if (atm_num == 1) {
    # for ATM1
    arima.model <- train %>% Arima(order=c(0,0,2), 
                                        seasonal = c(0,1,1), 
                                        lambda = lambda)
  } else if(atm_num == 2) {
    # for ATM2
    arima.model <- train %>% Arima(order=c(3,0,3), 
                                        seasonal = c(0,1,1), 
                                        include.drift = TRUE, 
                                        lambda = lambda,
                                        biasadj = TRUE)
  } else {
    # for ATM4
    arima.model <- train %>% Arima(order=c(0,0,1), 
                                    seasonal = c(2,0,0), 
                                    lambda = lambda)
  }
  
  # forecast
  hw.frct = forecast(hw.model, h = length(test))$mean
  ets.frct = forecast(ets.model, h = length(test))$mean
  arima.frct = forecast(arima.model, h = length(test))$mean
  
  # dataframe having rmse
  rmse = data.frame(RMSE=cbind(accuracy(hw.frct, test)[,2],
                                   accuracy(ets.frct, test)[,2],
                                   accuracy(arima.frct, test)[,2]))
  names(rmse) = c("Holt-Winters", "ETS", "ARIMA")
  # display rmse
  rmse
}
```


### ATM1

Seeing the time series plot, it is clear that there is a seasonality in the data. We can see increasing and decreasing activities over the weeks in below plot. From the ACF plot, we can see a slight decrease in every 7th lag due to trend. PACF plot shows some significant lags at the beginning. 


```{r atm1}
atm1.ts <- atm.new %>% filter(ATM=="ATM1") %>% select(Cash) %>% ts(frequency = 7)
ggtsdisplay(atm1.ts, main="ATM1 Cash Withdrawal", ylab="cash withdrawal", xlab="week")
```


From the above plots it is evident that the time series is non stationary, showing seasonality and will require differencing to make it stationary.


```{r}
ggsubseriesplot(atm1.ts, main="ATM1 Cash Withdrawal")
```

From the subseries plot, it is apparent that Tuesdays having highest mean of ash withdrawl while Saturdays being the lowest.

Next step is to apply BoxCox transformation. With $\lambda$ being 0.26, the resulting transformation does handle the variablity in time series as shown in below transformed plot.

```{r}
atm1.lambda <- BoxCox.lambda(atm1.ts)
atm1.ts.bc <- BoxCox(atm1.ts, atm1.lambda )
ggtsdisplay(atm1.ts.bc, main=paste("ATM1 Cash Withdrawal",round(atm1.lambda, 3)), ylab="cash withdrawal", xlab="week")
```

Next we will see the number of differences required for a stationary series and the number of differences required for a seasonally stationary series.


```{r}
# Number of differences required for a stationary series
ndiffs(atm1.ts.bc)
```

```{r}
# Number of differences required for a seasonally stationary series
nsdiffs(atm1.ts.bc)
```

It shows number of differences required for a seasonality stationary series is 1. Next step is to check kpss summary.

```{r}
atm1.ts.bc %>% diff(lag=7) %>% ur.kpss() %>% summary()
```


We can see the test statistic small and well within the range we would expect for stationary data. So we can conclude that the data are stationary.


```{r}
atm1.ts.bc %>% diff(lag=7) %>% ggtsdisplay()
```


The data is non-stationary with seasonality so there will be a seasonal difference of 1. Finally, the differencing of the data has now made it stationary. From the ACF plot, it is apparent now that there is a significant spike at lag 7 but none beyond lag 7.

Lets start with Holt-Winter’s additive model with damped trend since the seasonal variations are roughly constant through out the series.

```{r}
# Holt Winters with damped True
atm1.ts %>% hw(h=31, seasonal = "additive", lambda = atm1.lambda, damped = TRUE)
```

Next is to apply exponential smoothing method on this time series. It shows that the ETS(A, N, A) model best fits for the transformed ATM4, i.e. exponential smoothing with additive error, no trend component and additive seasonality.


```{r}
atm1.ts %>% ets(lambda = atm1.lambda )
```


Next we will find out the appropriate ARIMA model for this time series. The suggested model seems ARIMA(0,0,2)(0,1,1)[7].


```{r}
atm1.fit3 <- atm1.ts %>% auto.arima(lambda = atm1.lambda )
atm1.fit3
```


Next is to see residuals time series plot which shows residuals are being near normal with mean of the residuals being near to zero. Also there is no significant autocorrelation that confirms that forecasts are good.

```{r}
checkresiduals(atm1.fit3)
```

Let's plot the forecast for all the considered models above which will shows a nice visual comparison. it will also show a zoomed in plot to have a clearer view.


```{r}
atm.forecast(atm1.ts)
```



```{r}
model_accuracy(atm1.ts,1)
```


### ATM2


From the time series plot, it is apparent that there is a seasonality in the data but dont see a trend over the period. ACF shows teh significant lags at 7,14 and 21 confirming seasonality. From the PACF, there are few significant lags at the beginning but others within critical limit. Overall, it is non stationary, having seasonality and would require differencing for it to become stationary.


```{r atm2}
atm2.ts <- atm.new %>% filter(ATM=="ATM2") %>% select(Cash) %>% ts(frequency = 7)
ggtsdisplay(atm2.ts, main="ATM2 Cash Withdrawal", ylab="cash withdrawal", xlab="week")
```

From the subseries plot, it is clear that Sunday is having highest mean for cash withdrawl while Saturday has the lowest.


```{r}
ggsubseriesplot(atm2.ts, main="ATM2 Cash Withdrawal")
```

Next step is to apply BoxCox transformation. With $\lambda$ being 0.72, the resulting transformation does handle the variablity in time series as shown in below transformed plot.


```{r}
atm2.lambda <- BoxCox.lambda(atm2.ts)
atm2.ts.bc <- BoxCox(atm2.ts, atm2.lambda )
ggtsdisplay(atm2.ts.bc, main=paste("ATM2 Cash Withdrawal",round(atm2.lambda, 3)), ylab="cash withdrawal", xlab="week")
```


```{r}
# Number of differences required for a stationary series
ndiffs(atm2.ts.bc)
```


```{r}
# Number of differences required for a seasonally stationary series
nsdiffs(atm2.ts.bc)
```

It shows number of differences required is 1 for boxcox transformed data.

```{r}
atm2.ts.bc %>% diff(lag=7) %>% ur.kpss() %>% summary()
```


We can see the test statistic small and well within the range we would expect for stationary data. So we can conclude that the data are stationary



```{r}
atm2.ts.bc %>% diff(lag=7) %>% ggtsdisplay()
```


First we will start with Holt-Winters damped method. Damping is possible with both additive and multiplicative Holt-Winters’ methods. This method often provides accurate and robust forecasts for seasonal data is the Holt-Winters method with a damped trend.


```{r}
# Holt Winters
atm2.ts %>% hw(h=31, seasonal = "additive", lambda = atm2.lambda, damped = TRUE)
```

Next is to apply exponential smoothing method on this time series. It shows that the ETS(A, N, A) model best fits for the transformed ATM4, i.e. exponential smoothing with additive error, no trend component and additive seasonality.

```{r}
# ETS
atm2.ts %>% ets(lambda = atm2.lambda)
```



We will now find out the appropriate ARIMA model for this time series. The suggested model seeems ARIMA(3,0,3)(0,1,1)[7] with drift.


```{r}
atm2.fit3 <- atm2.ts %>% auto.arima(lambda = atm2.lambda )
atm2.fit3
```


Next is to see residuals time series plot which shows residuals are being near normal with mean of the residuals being near to zero. Also there is no significant autocorrelation that confirms that forecasts are good.

```{r}
checkresiduals(atm2.fit3)
```

Next step is to plot the forecast for all the considered models above which will shows a nice visual comparison. it will also show a zoomed in plot to have a clearer view.


```{r}
atm.forecast(atm2.ts)
```


```{r}
model_accuracy(atm2.ts,2)
```


### ATM3


```{r atm3}
atm3.ts <- atm.new %>% filter(ATM=="ATM3") %>% select(Cash) %>% ts(frequency = 7)
autoplot(atm3.ts, main="ATM3 Cash Withdrawal", ylab="cash withdrawal", xlab="week")
```





### ATM4

Seeing the time series plot, it is apparent that there is seasonality in this series. ACF shows a decrease in every 7th lag. From the PACF, there are few significant lags at the beginning but others within critical limit. Overall, it is non stationary, having seasonality and might require differencing for it to become stationary.



```{r atm4}
atm4.ts <- atm.new %>% filter(ATM=="ATM4") %>% select(Cash) %>% ts(frequency = 7)
ggtsdisplay(atm4.ts, main="ATM4 Cash Withdrawal", ylab="cash withdrawal", xlab="week")
```

From the subseries plot, it is clear that Sunday is having highest mean for cash withdrawl while Saturday has the lowest.

```{r}
ggsubseriesplot(atm4.ts, main="ATM4 Cash Withdrawal")
```

Next step is to apply BoxCox transformation. With $\lambda$ being 0.45, the resulting transformation does handle the variablity in time series as shown in below transformed plot.


```{r}
atm4.lambda <- BoxCox.lambda(atm4.ts)
atm4.ts.bc <- BoxCox(atm4.ts, atm4.lambda )
ggtsdisplay(atm4.ts.bc, main=paste("ATM4 Cash Withdrawal",round(atm4.lambda, 3)), ylab="cash withdrawal", xlab="week")
```


```{r}
# Number of differences required for a stationary series
ndiffs(atm4.ts.bc)
```


```{r}
# Number of differences required for a seasonally stationary series
nsdiffs(atm4.ts.bc)
```


It shows number of differences required is 0 for boxcox transformed data. 


```{r}
atm4.ts.bc %>% ur.kpss() %>% summary()
```


We can see the test statistic small and well within the range we would expect for stationary data. So we can conclude that the data are stationary.


```{r}
atm4.ts.bc %>% ggtsdisplay()
```


First we will start with Holt-Winters damped method. Damping is possible with both additive and multiplicative Holt-Winters’ methods. This method often provides accurate and robust forecasts for seasonal data is the Holt-Winters method with a damped trend.


```{r}
# Holt Winters
atm4.ts %>% hw(h=31, seasonal = "additive", lambda = atm4.lambda, damped = TRUE)
```


Next is to apply exponential smoothing method on this time series. It shows that the ETS(A, N, A) model best fits for the  transformed ATM4, i.e. exponential smoothing with additive error, no trend component and additive seasonality.

```{r}
# ETS
atm4.ts %>% ets(lambda = atm4.lambda)
```


Next we will find out the appropriate ARIMA model for this time series. The suggested model seeems ARIMA(0,0,1)(2,0,0)[7] with non-zero mean.

```{r}
# Arima
atm4.fit3 <- atm4.ts %>% auto.arima(lambda = atm4.lambda)
atm4.fit3
```

Next is to see residuals time series plot which shows residuals are being near normal with mean of the residuals being near to zero. Also there is no significant autocorrelation that confirms that forecasts are good.

```{r}
checkresiduals(atm4.fit3)
```



Next is to plot the forecast for all the considered models above which will shows a nice visual comparison. it will also show a zoomed in plot to have a clearer view.

```{r}
atm.forecast(atm4.ts)
```




```{r}
model_accuracy(atm4.ts,4)
```

## Forecast May, 2010



# Part B - Forecasting Power


The dataset contains residential power usage for January 1998 until December 2013. Its monthly data from 1998 and power consumed is in KWH column. This dataset contains a total 192 records.


```{r warning=FALSE}

download.file(
  url="https://github.com/amit-kapoor/data624/blob/main/Project1/ResidentialCustomerForecastLoad-624.xlsx?raw=true", 
  destfile = temp.file, 
  mode = "wb", 
  quiet = TRUE)
power.data <- read_excel(temp.file, skip=0, col_types = c("numeric","text","numeric"))

head(power.data)
```

## Exploratory Analysis


Seeing the plot closely, it is apparent that there is an outlier and a missing entry too. We will use the tsclean function to take care of missing entry and outlier in the data. Other than this data seems to be good shape. 


```{r}
power.data$`YYYY-MMM` <- paste0(power.data$`YYYY-MMM`,"-01")
power.data$Date <- lubridate::ymd(power.data$`YYYY-MMM`)

ggplot(power.data, aes(x=Date, y=KWH )) + 
  geom_line(color="darkblue")
```


## Data Cleaning

We will first create the time series of given data and then perform tsclean function.


```{r}
power.ts <- ts(power.data$KWH, start=c(1998, 1), frequency = 12)
head(power.ts)
```


```{r}
power.ts %>% summary()
```


```{r}
power.ts <- tsclean(power.ts)
power.ts %>% summary()
```


It is apparent that tsclean did take care of NA's and outlier in the data.

## Time Series


So far we have analyzed the data and perform data cleaning to handle missing and outlier data. In this section we will delve into the time series and see the models that perform best for prediction.


```{r}
ggtsdisplay(power.ts, main="Residential Power Usage", ylab="Power Used", xlab="Month")
```

From the above time series plot, it is evident that seasonality exists in the data. We dont see a trend in the data. ACF plot sgows the auto correlation and PACF shows few significant lags in the beginning. Overall, it shows seasonality and non stationary data. It could require differencing to make it stationary which will be confirmed in further steps.



```{r}
ggsubseriesplot(power.ts, main="Pwer Usage Subseries Plot", ylab="Power Used")
```



```{r}
ggseasonplot(power.ts, polar=TRUE, main="Power Usage Seasonal Plot", ylab="Power Used")
```

The seasonal plots above shows a decline in power usage from Jan to May, increase till Aug and then decline in Nov. Aug is the month of most power consumption.


```{r}
gglagplot(power.ts )
```

In the above lagplot, colors show different month. The lines connect points in chronological order. The relationship is strongly positive at lag 12, reflecting the strong seasonality in the data.

Next step is to apply Box-Cox transformation and check the transformed data.


```{r}
powerts.lambda <- BoxCox.lambda(power.ts)
power.ts.bc <- BoxCox(power.ts, powerts.lambda )
ggtsdisplay(power.ts, main=paste("Residential Power Usage",round(powerts.lambda, 3)), ylab="Power Used", xlab="Month")

```

The Box-Cox transformation above did handle the variation in the data with $\lambda$ as 0.144 and appears stable now. Next we see that Number of differences required for a stationary and seasonally stationary series are 1.



```{r}
# Number of differences required for a stationary series
ndiffs(power.ts.bc)
```


```{r}
# Number of differences required for a seasonally stationary series
nsdiffs(power.ts.bc)
```


It shows number of differences required is 1 for boxcox transformed data. 


```{r}
power.ts.bc %>% diff(lag=12) %>% ur.kpss() %>% summary()
```


We can see the test statistic small and well within the range we would expect for stationary data. So we can conclude that the data are stationary.


```{r}
power.ts.bc %>% 
  diff(lag=12) %>% 
  ggtsdisplay(main="Residential Power Usage", ylab="Power Used", xlab="Month")
```


Now we will apply four models in this time series: Holt Winters additive with damped True, Holt Winters multiplicative with damped True, exponential smoothing and arima.  First we will start with Holt-Winters damped method. Damping is possible with both additive and multiplicative Holt-Winters’ methods. This method often provides accurate and robust forecasts for seasonal data is the Holt-Winters method with a damped trend.


```{r}
# Holt Winters additive with damped True
power.ts %>% hw(h=31, seasonal = "additive", lambda = powerts.lambda, damped = TRUE)
```



```{r}
# Holt Winters multiplicative with damped True
power.ts %>% hw(h=31, seasonal = "multiplicative", damped = TRUE)
```


Next model to ETS: Exponential Smoothing methods.

```{r}
# exponential smoothing
power.ts %>% ets(lambda = powerts.lambda, biasadj = TRUE)
```


We can see here that the ets model that best describes the data is **ETS(A,Ad,A)** i.e. exponential smoothing with additive error, additive damped trend and additive seasonality.

Next we will find the best Arima model that fits this time series data.



```{r}
power.fit4 <- power.ts %>% auto.arima(lambda = powerts.lambda, biasadj = TRUE)
power.fit4
```


The best Arima model comes out is **ARIMA(0,0,1)(2,1,0)[12]** with drift.



```{r}
checkresiduals(power.fit4)
```



Next is to plot the forecasts using all of 4 models described above: HW additive, HW multiplicative, ets and arima. For this, we will create a generic function which will accept the time series and plot the forecast for all these 4 models.  There is also a zoomed in plot of the forecast for better clarity.


```{r pow-fcst}

# function to plot forecast(s)
power.forecast <- function(timeseries) {
  # lambda value
  lambda <- BoxCox.lambda(timeseries)
  
  # models for forecast
  hwa.model <- timeseries %>% hw(h=12, seasonal = "additive", lambda = lambda, damped = TRUE)
  hwm.model <- timeseries %>% hw(h=12, seasonal = "multiplicative", damped = TRUE)
  ets.model <- timeseries %>% ets(lambda = lambda )
  arima.model <- timeseries %>% auto.arima(lambda = lambda, biasadj = TRUE)
  
  # forecast
  pow.hwa.fcst <- forecast(hwa.model, h=12)
  pow.hwm.fcst <- forecast(hwm.model, h=12)
  pow.ets.fcst <- forecast(ets.model, h=12)
  pow.arima.fcst <- forecast(arima.model, h=12)
  
  # plot forecasts
  p1 <- autoplot(timeseries) + 
    autolayer(pow.hwa.fcst, PI=FALSE, series="Holt-Winters Additive") + 
    autolayer(pow.hwm.fcst, PI=FALSE, series="Holt-Winters Multiplicative") + 
    autolayer(pow.ets.fcst, PI=FALSE, series="ETS") + 
    autolayer(pow.arima.fcst, PI=FALSE, series="ARIMA") + 
    theme(legend.position = "top") + 
    ylab("Power Used") 
  
  # zoom in plot
  p2 <- p1 + 
    labs(title = "Zoom in ") + 
    xlim(c(2012,2015))
  
  grid.arrange(p1,p2,ncol=1)

}
```


Lets plot the forecast now using the above function for power usage.



```{r}
power.arima.fcst <- forecast(power.fit4, h=12)
power.forecast(power.ts)
```


Now we will check the accuracy of all 4 models. Again for this purpose, we have created a function that accepts the timeseries and divide the data for training and testing. In this function we will first divide the data for training and testing, train all models with train set and then find out RMSE using test data.


```{r rmse1}
powm_accuracy <- function(timeseries) {
  # lambda value
  lambda <- BoxCox.lambda(timeseries)
  
  # split the data to train and test
  train <- window(timeseries, end=c(2009, 12))
  test <- window(timeseries, start=2010)
  
  # models for forecast
  hwa.model <- train %>% hw(h=length(train), seasonal = "additive", lambda = lambda, damped = TRUE)
  hwm.model <- train %>% hw(h=length(train), seasonal = "multiplicative", damped = TRUE)
  ets.model <- train %>% ets(model="AAA", lambda = lambda, biasadj = TRUE)
  arima.model <- train %>% Arima(order=c(0,0,1), 
                                        seasonal = c(2,1,0), 
                                        include.drift = TRUE, 
                                        lambda = lambda,
                                        biasadj = TRUE)
  
  
  # forecast
  hwa.frct = forecast(hwa.model, h = length(test))$mean
  hwm.frct = forecast(hwm.model, h = length(test))$mean
  ets.frct = forecast(ets.model, h = length(test))$mean
  arima.frct = forecast(arima.model, h = length(test))$mean
  
  # dataframe having rmse
  rmse = data.frame(RMSE=cbind(accuracy(hwa.frct, test)[,2],
                               accuracy(hwm.frct, test)[,2],
                               accuracy(ets.frct, test)[,2],
                              accuracy(arima.frct, test)[,2]))
  names(rmse) = c("Holt-Winters Additive", "Holt-Winters Multiplicative", "ETS", "ARIMA")
  
  # display rmse
  rmse
}
```



```{r}
powm_accuracy(power.ts)
```


Thus **ARIMA(0,0,1)(2,1,0)[12]** with drift has been the best model to describe the gicen time series.


## Forecast 2014

In this last step we will perform the forecast for 2014.


```{r}
pow.fcst.date <- seq(as.Date('2014-01-01'), as.Date('2014-12-01'), by="month") %>% format("%Y-%b")

write.xlsx(data.frame('DateTime' = pow.fcst.date, 'Waterflow'= power.arima.fcst$mean), 
           "Kapoor_data624_pow_forecasts.xlsx")

```




```{r warning=FALSE}
pow.fcst.ak <- read_excel("Kapoor_data624_pow_forecasts.xlsx", skip=0, col_types = c("text","numeric"))
pow.fcst.ak %>% 
  kbl() %>% 
  kable_paper() %>% 
  scroll_box(width = "500px", height = "200px")
```









# Part C - Waterflow Pipe

In part C we have been provided 2 datasets for waterflow pipes. These are simple 2 columns sets, however they have different time stamps. Each dataset contains 1000 records and has no missing data.

```{r warning=FALSE}

download.file(url="https://github.com/amit-kapoor/data624/blob/main/Project1/Waterflow_Pipe1.xlsx?raw=true", 
              destfile = temp.file, 
              mode = "wb", 
              quiet = TRUE)
pipe1.data <- read_excel(temp.file, skip=0, col_types = c("date","numeric"))

download.file(url="https://github.com/amit-kapoor/data624/blob/main/Project1/Waterflow_Pipe2.xlsx?raw=true", 
              destfile = temp.file, 
              mode = "wb", 
              quiet = TRUE)

pipe2.data <- read_excel(temp.file, skip=0, col_types = c("date","numeric"))

```


It is apparent here that the data is recorded on different time intervals for pipe1 and pipe2. For pipe1 it shows records for multiple time intervals within an hour i.e. not evenly distributed while for pipe2 it seems hourly recorded. 


```{r}
pipe1.data %>% 
  kbl() %>% 
  kable_paper() %>% 
  scroll_box(width = "500px", height = "200px")
```


```{r}
pipe2.data %>% 
  kbl() %>% 
  kable_paper() %>% 
  scroll_box(width = "500px", height = "200px")
```


## Exploratory Analysis


In this first step for analysis, We will check the daily frequency count for both datasets. It is apparent here that number of records daily for pipe1 is not evenly distributed and there are more recording per hour for pipe1. 


```{r}
p1 <- pipe1.data 
p1$Date <- ymd_hms(p1$`Date Time`)
p1$Date <- as.Date(p1$Date)

p1 <- p1 %>% group_by(Date) %>% summarise("Pipe1 Records count"=n())

p2 <- pipe2.data 
p2$Date <- ymd_hms(p2$`Date Time`)
p2$Date <- as.Date(p2$Date)

p2 <- p2 %>% group_by(Date) %>% summarise("Pipe2 Records count"=n())

merge(p1,p2,by="Date")
```

It is evident here that pipe1 dataset has an uneven count distribution for given date while pipe2 has even count daily.


## Data Cleaning

In this step we will make pipe1 data evenly distributed (per hour) for every day. As mentioned in the problem *for multiple recordings within an hour, take the mean*, we will take mean of records per hour and then recreate the `Date Time` column. Next we will do the same count comparison that was done earlier and check number of records per day.



```{r}
pipe1.data <- pipe1.data %>% 
  mutate(Date=date(`Date Time`), Hour=hour(`Date Time`)) %>% 
  group_by(Date, Hour) %>% 
  summarise(WaterFlow=mean(WaterFlow)) %>% 
  ungroup() %>% 
  mutate(`Date Time`=ymd_h(paste(Date, Hour))) %>% 
  select(`Date Time`, WaterFlow)

# check the number of records per hour daily
p1 <- pipe1.data 
p1$Date <- ymd_hms(p1$`Date Time`)
p1$Date <- as.Date(p1$Date)

p1 <- p1 %>% group_by(Date) %>% summarise("Pipe1 Records count"=n())

p2 <- pipe2.data 
p2$Date <- ymd_hms(p2$`Date Time`)
p2$Date <- as.Date(p2$Date)

p2 <- p2 %>% group_by(Date) %>% summarise("Pipe2 Records count"=n())

merge(p1,p2,by="Date")
```


## Time Series

In this section we will create the timeseries out of both datasets and check out the best model that will eventually provide a week forward forecast. 


### Pipe1

We will start with pipe1 dataset by creating its time series, plot the waterflow data and check the trend and seasonality, if exist.

```{r}
pipe1.ts <- ts(pipe1.data$WaterFlow)
pipe1.ts %>% summary()
```


```{r}
ggtsdisplay(pipe1.ts, main="Pipe1 Waterflow", ylab="Water")
```

By seeing the time series, it is apparent there is no seasonality or trend in the data. The ACF and PACF plots shows white noise. It depicts that the time series is stationary and doesnt require diffencing.


```{r}
pipe1.lambda <- BoxCox.lambda(pipe1.ts)
pipe1.ts.bc <- BoxCox(pipe1.ts, pipe1.lambda )
ggtsdisplay(pipe1.ts.bc, main=paste("Pipe1 Waterflow",round(pipe1.lambda, 3)), ylab="Water")

```


```{r}
# Number of differences required for a stationary series
ndiffs(pipe1.ts.bc)
```



```{r}
pipe1.ts.bc %>% ur.kpss() %>% summary()
```

We can see the test statistic small and well within the range we would expect for stationary data. So we can conclude that the data are stationary.

```{r}
# ets
pipe1.ets.model <- pipe1.ts %>% ets(lambda = pipe1.lambda)
pipe1.ets.model
```

We can see here that the ets model that best describes the data is ETS(A,N,N) i.e. exponential smoothing with additive error, no trend and no seasonality.

Next we will find the best Arima model that fits this time series data.

```{r}
# arima
pipe1.arima.model <- pipe1.ts %>% auto.arima(lambda = pipe1.lambda)
pipe1.arima.model
```

The best Arima model comes out is ARIMA(0,0,0) with non-zero mean.


Next is to plot the forecasts using both the models described above: ets and arima. For this, we will create a generic function which will accept the time series and pipe number and plot the forecast using both the models. 


```{r pipe-fcst}
# function to plot forecast(s)
pipe.forecast <- function(timeseries, pipe_num) {
  # lambda value
  lambda <- BoxCox.lambda(timeseries)
  
  # models for forecast
  ets.model <- timeseries %>% ets(lambda = lambda)
  arima.model <- timeseries %>% auto.arima(lambda = lambda)
  
  # forecast h=24*7=168
  pipe.ets.fcst <- forecast(ets.model, h=168)
  #print(pipe.ets.fcst$mean)
  pipe.arima.fcst <- forecast(arima.model, h=168)
  #print(pipe.arima.fcst$mean)
  
  # plot forecasts
  p1 <- autoplot(timeseries) + 
    autolayer(pipe.ets.fcst, PI=FALSE, series="ETS") + 
    autolayer(pipe.arima.fcst, PI=FALSE, series="ARIMA") + 
    theme(legend.position = "top") + 
    ylab("Water") 
  
  # zoom in plot
  if (pipe_num == 1) {
    p2 <- p1 + 
      labs(title = "Zoom in ") + 
      xlim(c(225,325))
  } else {
    p2 <- p1 + 
      labs(title = "Zoom in ") + 
      xlim(c(990,1150))    
  }
  
  
  grid.arrange(p1,p2,ncol=1)

}
```

Lets plot the forecast for pipe1. The forecast is for every hour in a day for a week. 

```{r}
pipe1.ets.fcst <- forecast(pipe1.ets.model, h=168)
pipe1.arima.fcst <- forecast(pipe1.arima.model, h=168)
pipe.forecast(pipe1.ts, 1)
```

We can see that forecasts for both ETS and ARIMA models are almost on top of each other and for pipe1, the waterflow is forecasted to be 19.5548.

Now we will check the accuracy of both the models ETS and ARIMA. Again for this purpose, we have created a function that accepts the timeseries and a timebreak (for train and test data). In this function we will first divide the data for training and testing, train both models with train set and then find out RMSE using test data.



```{r pipe-rmse}
pipe_accuracy <- function(timeseries, time_break) {
  # lambda value
  lambda <- BoxCox.lambda(timeseries)
  
  # split the data to train and test
  train <- window(timeseries, end=time_break)
  test <- window(timeseries, start=time_break+1)
  
  # models for forecast
  ets.model <- train %>% ets(model="ANN", lambda = lambda, biasadj = TRUE)
  arima.model <- train %>% Arima(order=c(0,0,0), 
                                 lambda = lambda,
                                 biasadj = TRUE)
  
  # forecast
  ets.frct = forecast(ets.model, h = length(test))$mean
  arima.frct = forecast(arima.model, h = length(test))$mean
  
  # dataframe having rmse
  rmse = data.frame(RMSE=cbind(accuracy(ets.frct, test)[,2],
                              accuracy(arima.frct, test)[,2]))
  names(rmse) = c("ETS", "ARIMA")
  
  # display rmse
  rmse
}
```


By passing multiple time breaks for training and test data, we see below RMSEs.

```{r}
pipe_accuracy(pipe1.ts, 190)
```

```{r}
pipe_accuracy(pipe1.ts, 178)
```


```{r}
pipe_accuracy(pipe1.ts, 200)
```


It is evident here that the model best describes the pipe1 time series is **ETS(A,N,N) with Box-Cox transformation of 0.272**.



```{r}
checkresiduals(pipe1.ets.model)
```




### Pipe2

Similar to pipe1, we will create time series for pipe2, plot the waterflow data and check the trend and seasonality, if exist.


```{r}
pipe2.ts <- ts(pipe2.data$WaterFlow)
pipe2.ts %>% summary()
```



```{r}
ggtsdisplay(pipe2.ts, main="Pipe2 Waterflow", ylab="Water")
```


By seeing the time series, it is apparent there is no seasonality or trend in the data. The ACF and PACF plots shows few significant auto correlations.


```{r}
pipe2.lambda <- BoxCox.lambda(pipe2.ts)
pipe2.ts.bc <- BoxCox(pipe2.ts, pipe2.lambda )
ggtsdisplay(pipe2.ts.bc, main=paste("Pipe2 Waterflow",round(pipe2.lambda, 3)), ylab="Water")

```


```{r}
# Number of differences required for a stationary series
ndiffs(pipe2.ts.bc)
```


```{r}
pipe2.ts.bc %>% ur.kpss() %>% summary()
```


We can see the test statistic small and well within the range we would expect for stationary data. So we can conclude that the data are stationary.


```{r}
# ets
pipe2.ets.model <- pipe2.ts %>% ets(lambda = pipe2.lambda)
pipe2.ets.model
```


We can see here that the ets model that best describes the data is ETS(A,N,N) i.e. exponential smoothing with additive error, no trend and no seasonality.

Next we will find the best Arima model that fits this time series data.


```{r}
# arima
pipe2.arima.model <- pipe2.ts %>% auto.arima(lambda = pipe2.lambda)
pipe2.arima.model
```


The best Arima model for pipe1 comes out is ARIMA(0,0,0) with non-zero mean, similar to pipe1. 

Next we will see the forecast plots using both these models for pipe1. 



```{r}
pipe2.ets.fcst <- forecast(pipe2.ets.model, h=168)
pipe2.arima.fcst <- forecast(pipe2.arima.model, h=168)
pipe.forecast(pipe2.ts, 2)
```

Similar to pipe1, ee can see that forecasts in this case for both ETS and ARIMA models are almost on top of each other and for pipe2, the waterflow is forecasted to be 39.012.

By passing multiple time breaks for training and test data, we see below RMSEs.


```{r}
pipe_accuracy(pipe2.ts, 969)
```


```{r}
pipe_accuracy(pipe2.ts, 850)
```


```{r}
pipe_accuracy(pipe2.ts, 877)
```

It is evident here that the model best describes the pipe1 time series is ARIMA(0,0,0)  with Box-Cox transformation of 0.849.


```{r}
checkresiduals(pipe2.arima.model)
```



## Forecast a week forward

Finally lets do the forecast for both pipes (pipe1 and pipe2) using the above selected best models and save the corresponding xlsx.


```{r}
pipe1.fcst.date <- seq(ymd_hm('2015-11-01 24:00'), ymd_hm('2015-11-08 23:00'), by="hour")
pipe2.fcst.date <- seq(ymd_hm('2015-12-03 17:00'), ymd_hm('2015-12-10 16:00'), by="hour")

write.xlsx(data.frame('DateTime' = pipe1.fcst.date, 'Waterflow'= pipe1.ets.fcst$mean), 
           "Kapoor_data624_pipe1_forecasts.xlsx")

write.xlsx(data.frame('DateTime' = pipe2.fcst.date, 'Waterflow'= pipe2.arima.fcst$mean), 
           "Kapoor_data624_pipe2_forecasts.xlsx")
```




```{r warning=FALSE}
pipe1.fcst.ak <- read_excel("Kapoor_data624_pipe1_forecasts.xlsx", skip=0, col_types = c("date","numeric"))
pipe1.fcst.ak %>% 
  kbl() %>% 
  kable_paper() %>% 
  scroll_box(width = "500px", height = "200px")
```


```{r warning=FALSE}
pipe2.fcst.ak <- read_excel("Kapoor_data624_pipe2_forecasts.xlsx", skip=0, col_types = c("date","numeric"))
pipe2.fcst.ak %>% 
  kbl() %>% 
  kable_paper() %>% 
  scroll_box(width = "500px", height = "200px")
```








