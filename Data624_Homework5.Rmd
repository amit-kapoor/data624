---
title: "Data624 - Homework5"
author: "Amit Kapoor"
date: "3/7/2021"
output:
  html_document:
    highlight: pygments
    number_sections: no
    theme: flatly
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r loadData, include=FALSE}
# Libraries
library(fpp2)
```

# Exercise 7.1
Consider the `pigs` series — the number of pigs slaughtered in Victoria each month.

```{r}
str(pigs)
```



## a)
Use the `ses()` function in R to find the optimal values of $\alpha$ and $\ell_0$, and generate forecasts for the next four months.


```{r ses-pigs}
# Using ses for pigs
pigs_ses <- ses(pigs, h=4)

#summary
summary(pigs_ses)
```

Above summary shows the the optimal values of $\alpha$ and $\ell_0$ are 0.2971 and 77260.0561 respectively. Using these values forecast is generated for next 4 months.

Next plot shows the forecast from simple exponential smoothing. Also one-step-ahead fitted values are plotted with the data over the period.

```{r}
autoplot(pigs_ses) +
  autolayer(fitted(pigs_ses), series="Fitted") +
  ylab("Number of pigs slaughtered in Victoria")
```



## b)
Compute a 95% prediction interval for the first forecast using $\hat{y} \pm 1.96 \sigma$ where $\sigma$ is the standard deviation of the residuals. Compare your interval with the interval produced by R.

```{r formula}
# 95% prediction interval for the first forecas
sd <- sd(residuals(pigs_ses))
ci95 <- c(lower = pigs_ses$mean[1] - 1.96*sd, upper = pigs_ses$mean[1] + 1.96*sd)
ci95
```


```{r byR}
# By R
ci95_R <- c(pigs_ses$lower[1, "95%"], pigs_ses$upper[1, "95%"])
names(ci95_R) <- c("lower", "upper")
ci95_R
```

It appears the 95% prediction interval calculated by R is a little wider than the one given by the formula.


# Exercise 7.5
Data set `books` contains the daily sales of paperback and hardcover books at the same store. The task is to forecast the next four days’ sales for paperback and hardcover books.

```{r books}
head(books)
```




## a)
Plot the series and discuss the main features of the data.

```{r plot-books}
# plot series
autoplot(books) + 
  labs(title = "Daily Sales of Paperback and Hardcover Books")
```

The series has an upward trend but don't see any seasonality or cyclicity in the plot. Also its only a 30 days of data so difficult to speak about seasonality. Another observation is hardcover sales in better than paperback.

## b)
Use the `ses()` function to forecast each series, and plot the forecasts.

```{r ses-books}
# Using ses for books
pb_ses <- ses(books[, 'Paperback'], h=4)
hc_ses <- ses(books[, 'Hardcover'], h=4)

autoplot(books) +
  autolayer(pb_ses, series="Paperback", PI=FALSE) +
  autolayer(hc_ses, series="Hardcover", PI=FALSE) + 
  labs(title = "Daily Sales of Paperback and Hardcover Books (ses)")
```

The simple exponential smoothing plot above shows flat forecast and doesnt appear to capture upward trend.

## c)
Compute the RMSE values for the training data in each case.

```{r rmse1}
# RMSE for paperback
round(accuracy(pb_ses)[2], 2)
```


```{r rmse2}
# RMSE for hardcover
round(accuracy(hc_ses)[2], 2)
```

RMSE of hardcover for the training data is slightly better than of paperback.


# Exercise 7.6
We will continue with the daily sales of paperback and hardcover books in data set books.

## a)
Apply Holt’s linear method to the `paperback` and `hardback` series and compute four-day forecasts in each case.


```{r holt}
pb_holt <- holt(books[, 'Paperback'], h=4)
hc_holt <- holt(books[, 'Hardcover'], h=4)

autoplot(books) +
  autolayer(pb_holt, series="Paperback", PI=FALSE) +
  autolayer(hc_holt, series="Hardcover", PI=FALSE) + 
  labs(title = "Daily Sales of Paperback and Hardcover Books (holt)")
```

Holt's linear forecast seems better as it is able to capture the upward trend of time series.

## b)
Compare the RMSE measures of Holt’s method for the two series to those of simple exponential smoothing in the previous question. (Remember that Holt’s method is using one more parameter than SES.) Discuss the merits of the two forecasting methods for these data sets.


```{r rmse1-h}
# RMSE for paperback - holt
round(accuracy(pb_holt)[2], 2)
```


```{r rmse2-h}
# RMSE for hardcover - holt
round(accuracy(hc_holt)[2], 2)
```

The RMSEs for paperback and hardcover books sale are improved using holt's linear method as compared from simple exponential smoothing method. It happens since holt extended simple exponential smoothing to allow the forecasting of data with a trend and we see in above plot of holts, capturing upward trend.


## c)
Compare the forecasts for the two series using both methods. Which do you think is best?


```{r comp}
# ses and holt comparison for paperback
s1 <- autoplot(pb_ses) + 
  ylab("paperback book sales") + 
  labs(title = "ses forecast - paperback")
h1 <- autoplot(pb_holt) + 
  ylab("paperback book sales") + 
  labs(title = "holt forecast - paperback")

# ses and holt comparison for hardcover
s2 <- autoplot(hc_ses) + 
  ylab("hardcover book sales") + 
  labs(title = "ses forecast - hardcover")
h2 <- autoplot(hc_holt) + 
  ylab("hardcover book sales") + 
  labs(title = "holt forecast - hardcover")

gridExtra::grid.arrange(s1, h2, s1,h2, nrow=2, ncol=2)
```

Compare the forecasts for the two series using the two, Holt's method is better than simple exponential smoothing method sith holt extended simple exponential smoothing to allow the forecasting of data with a trend. The RMSE is smaller for holt method.



## d)
Calculate a 95% prediction interval for the first forecast for each series, using the RMSE values and assuming normal errors. Compare your intervals with those produced using `ses` and `holt`.

```{r rmse}
# 95% prediction interval for the first forecast
df <- data.frame(
  Pred_Int = c("Paperback-SES", "Paperback-Holt","Hardcover-SES", "Hardcover-Holt"),
  lower = c(pb_ses$mean[1] - 1.96*accuracy(pb_ses)[2],
            pb_holt$mean[1] - 1.96*accuracy(pb_holt)[2],
            hc_ses$mean[1] - 1.96*accuracy(hc_ses)[2],
            hc_holt$mean[1] - 1.96*accuracy(hc_holt)[2]), 
  upper = c(pb_ses$mean[1] + 1.96*accuracy(pb_ses)[2],
            pb_holt$mean[1] + 1.96*accuracy(pb_holt)[2],
            hc_ses$mean[1] + 1.96*accuracy(hc_ses)[2],
            hc_holt$mean[1] + 1.96*accuracy(hc_holt)[2])
)

df
```


```{r holt-ses}
df2 <- data.frame(
  Pred_Int = c("Paperback-SES", "Paperback-Holt","Hardcover-SES", "Hardcover-Holt"),
  lower = c(pb_ses$lower[1, "95%"],
            pb_holt$lower[1, "95%"],
            hc_ses$lower[1, "95%"],
            hc_holt$lower[1, "95%"]), 
  upper = c(pb_ses$upper[1, "95%"],
            pb_holt$upper[1, "95%"],
            hc_ses$upper[1, "95%"],
            hc_holt$upper[1, "95%"])
)

df2
```

From the interval range above, it is apparent that the interval calculated using RMSE is slightly narrower than from R using holt ans ses methods.

# Exercise 7.7
For this exercise use data set `eggs`, the price of a dozen eggs in the United States from 1900–1993. Experiment with the various options in the `holt()` function to see how much the forecasts change with the damped trend, or with a Box-Cox transformation. Try to develop an intuition of what each argument is doing to the forecasts.

[Hint: use `h = 100` when calling `holt()` so you can clearly see the differences between the various options when plotting the forecasts.]

Which model gives the best RMSE?

```{r eggs}
head(eggs)
```


```{r ap-eggs}
autoplot(eggs) + 
  labs(title = "Price of a dozen eggs in the United States from 1900–1993")
```

The time series shows the downward trend. It has the frequency as 1 that shows yearly record. We will perform forecast using below methods:

* Holt's
* Holts with damped trend
* Box-Cox transformation
* Box-Cox with damped trend
* Exponential
* Exponential with damped trend

```{r warning=FALSE}
h <- 100

# holts 
eggs_holt <- holt(eggs, h=h)
# holts with damped trend
eggs_damped <- holt(eggs, h=h, damped = T)
# Box-Cox transformation
eggs_boxcox <- holt(eggs, h=h, lambda = "auto")
# Box-Cox with damped trend
eggs_boxcox_d <- holt(eggs, h=h, lambda = "auto", damped = T)
# exponential
eggs_exp <- holt(eggs, h=h, exponential = T)
# exponential with damped trend
eggs_exp_d <- holt(eggs, h=h, exponential = T, damped = T)
```



```{r holts}
# Forcast from holt's
autoplot(eggs_holt)
```


```{r damped-holt}
# Forecast from damped holt's
autoplot(eggs_damped)
```


```{r boxcox-holts}
# forecast from boxcox transformation
autoplot(eggs_boxcox) + 
  labs(title = "Forecast using BoxCox Transformation")
```

```{r}
# forecast from damped boxcox transformation
autoplot(eggs_boxcox_d) + 
  labs(title = "Forecast using Damped BoxCox Transformation")
```

```{r}
# Forecast using exponential trend
autoplot(eggs_exp)
```

```{r}
# Forecast using exponential trend and damped
autoplot(eggs_exp_d)
```


```{r eggs-rmse}

rmse_eggdf <- data.frame(
  Method = c("Holt's", "Damped Holt's","BoxCox", "Damped BoxCox", "Exponential", "Damped Exponential"),
  RMSE = c(accuracy(eggs_holt)[2],
           accuracy(eggs_damped)[2],
           accuracy(eggs_boxcox)[2],
           accuracy(eggs_boxcox_d)[2], 
           accuracy(eggs_exp)[2],
           accuracy(eggs_exp_d)[2])
)

rmse_eggdf
```

Analyzing all the RMSEs above for all the methods, it appears BoxCox transformation is the lowest (=26.39376). From all the 6 graphs above it is clear too that BoxCox transformation forecast captures the decline trend and good enough among all. 


# Exercise 7.8 
Recall your retail time series data (from Exercise 3 in Section 2.10).

```{r retail}
retaildata <- readxl::read_excel("retail.xlsx", skip=1)
myts <- ts(retaildata[,"A3349627V"], frequency=12, start=c(1982,4))

```



## a)
Why is multiplicative seasonality necessary for this series?

```{r ap-retail}
autoplot(myts) + 
  labs(title="Retail Sales")
```

The multiplicative method is preferred when the seasonal variations are changing proportional to the level of the series. It appears in above graph that the variability in the series is increasing over years therefore multiplicative seasonality is necessary for the series.

## b)
Apply Holt-Winters’ multiplicative method to the data. Experiment with making the trend damped.


```{r hw}
# multiplicative
myts_hw <- hw(myts, seasonal = "multiplicative")
summary(myts_hw)
```

```{r hw-d}
# multiplicative damped
myts_hwd <- hw(myts, seasonal = "multiplicative", damped = T)
summary(myts_hwd)
```


```{r ap-mul-d}
autoplot(myts) + 
  autolayer(myts_hw, PI=F, series='Multiplicative') +
  autolayer(myts_hwd, PI=F, series='Multiplicative with damped trend') + 
  theme(legend.position = "top") + 
  ylab("Retail Sales")
```

Seeing the forecast, it appears multiplicative damped forecast the trend increases slowly as compared to only multiplicative one.

## c)
Compare the RMSE of the one-step forecasts from the two methods. Which do you prefer?

```{r}
rmse_retdf <- data.frame(
  Method = c("Mulitplicative", "Damped Mulitplicative"),
  RMSE = c(accuracy(myts_hw)[2],
           accuracy(myts_hwd)[2])
)

rmse_retdf
```
Comparing RMSEs for both these methods, it is apparent that multiplicative with damped method is better than multiplicative only.

## d)
Check that the residuals from the best method look like white noise.

```{r res}
checkresiduals(myts_hw)
```

For white noise series, we expect each autocorrelation to be close to zero. If one or more large spikes are outside these bounds, or if substantially more than 5% of spikes are outside these bounds, then the series is probably not white noise. Ljung-Box test result and ACF plot show that the residuals aren’t white noise.


## e)
Now find the test set RMSE while training the model to the end of 2010. Can you beat the seasonal naive approach from Exercise 8 in Section 3.7?

```{r testset}
myts_train <- window(myts, end=c(2010, 12))
myts_test <- window(myts, start = 2011)

myts_train_hw <- hw(myts_train, h=36, seasonal = "multiplicative")
myts_train_hwd <- hw(myts_train, h=36, seasonal = "multiplicative", damped = T)
myts_train_sn <- snaive(myts_train, h=36)

ap <- autoplot(myts_train) + 
  autolayer(myts_train_hw,PI=FALSE, series='Multiplicative') +
  autolayer(myts_train_hwd, PI=FALSE, series='Multiplicative with damped trend') + 
  autolayer(myts_train_sn, PI=FALSE, series='Seasonal Naive') + 
  autolayer(myts_test, PI=FALSE, series='Test set') + 
  theme(legend.position = "top") + 
  ylab("Retail Sales")

ap
```


```{r zoom, warning=FALSE}
ap + labs(title = "Zoom in - 2011 to 2014") + xlim(c(2011,2014))
```



```{r a1}
# RMSE- Holt-Winters’ multiplicative method 
accuracy(myts_train_hw, myts_test)[,2]
```

```{r a2}
# RMSE- Holt-Winters’ multiplicative method with damped trend
accuracy(myts_train_hwd, myts_test)[,2]
```

```{r a3}
# RMSE- Seasonal Naive
accuracy(myts_train_sn, myts_test)[,2]
```


Seeing the RMSEs for training and test sets, it seems Holt-Winters’ multiplicative method with damping does fit the timeseries best among all. Therfore Holt-Winters’ seems far more better than Seasonal Naive.




# Exercise 7.9
For the same retail data, try an STL decomposition applied to the Box-Cox transformed series, followed by ETS on the seasonally adjusted data. How does that compare with your best previous forecasts on the test set?

stlf() function is used to build the model that accepts time series, apply box cox transformation with ETS model.

```{r sts-etl}
# STL and ETS
# ets to use for forecasting the seasonally adjusted series
# ZZN - N=none, A=additive, M=multiplicative and Z=automatically
# lambda=auto - Box-Cox transformation parameter
myts_t_stlets <- stlf(myts_train, 
                      h=36, 
                      method="ets", 
                      etsmodel="ZZN", 
                      lambda = "auto", 
                      allow.multiplicative.trend = TRUE)

ap_stl <- autoplot(myts_train) + 
  autolayer(myts_t_stlets, PI=FALSE, series='STS and ETL') +
  autolayer(myts_train_hw, PI=FALSE, series='Multiplicative') + 
  autolayer(myts_train_hwd, PI=FALSE, series='Multiplicative with damped trend') + 
  autolayer(myts_test, PI=FALSE, series='Test set') + 
  theme(legend.position = "top") + 
  ylab("Retail Sales")

ap_stl
```

```{r zoom2, warning=FALSE}
# zoom in
ap_stl + labs(title = "Zoom in - 2011 to 2014") + xlim(c(2011,2014))
```


```{r a4}
# RMSE- stl ets
accuracy(myts_t_stlets, myts_test)[,2]
```

```{r a5}
# RMSE- Holt-Winters’ multiplicative method with damped trend
accuracy(myts_train_hwd, myts_test)[,2]
```

```{r a6}
# RMSE- Holt-Winters’ multiplicative method
accuracy(myts_train_hw, myts_test)[,2]
```


From the results, it is clear it seems Holt-Winters’ multiplicative method with damping still fits the timeseries better than STL decomposition applied to the Box-Cox transformed series, followed by ETS on the seasonally adjusted data. RMSE difference shows the same results.













